在从Web上爬取数据以后，我们需要对数据进行保存，以便后续进一步的数据分析。由于数据库的读写速度很快，所以一般爬虫数据会存储在数据库中，除此之外，简单的爬虫数据可以保存在文本，Json文件或者Excel文件中。本文为学习[崔庆才爬虫教程](https://germey.gitbooks.io/python3webspider/content/5.1.1-TXT%E6%96%87%E6%9C%AC%E5%AD%98%E5%82%A8.html)总结。

### 1.保存数据至TXT文本中

跟其他编程语言一样，保存txt文本，只需打开一个文本，指定读写方式，写入数据，然后关闭即可。在Python中还可以用`with open`方式来简化。

```python
 with open('save_data_to_txt.txt', 'a', encoding='utf-8') as f:
        f.write('\n'.join([question, author, answer]))
```

下面给一个爬取知乎数据的例子，爬取知乎探索页面，获取**问题**，**作者**和**答案**三部分。

```python
import requests
from pyquery import PyQuery as pq

base_url = 'https://www.zhihu.com/explore'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
}

html = requests.get(base_url,headers=headers).text
doc = pq(html)
items = doc('.explore-tab .feed-item').items()

for item in items:
    question = item.find('h2').text()
    author = item.find('.author-link-line').text()
    answer = pq(item.find('.content').html()).text()
    with open('zhihu-explore.txt', 'a', encoding='utf-8') as f:
        f.write('\n'.join([question, author, answer]))
        f.write('\n' + '=' * 50 + '\n')
```

注：文本打开方式可以随意组合如下。

- r，以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。
- rb，以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。
- r+，打开一个文件用于读写。文件指针将会放在文件的开头。
- rb+，以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。
- w，打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
- wb ，以二进制格式打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
- w+， 打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
- wb+，以二进制格式打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。
- a，打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。
- a+，打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。
- ab+，以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。
